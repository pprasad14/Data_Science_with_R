install.packages("decision.tree")
install.packages("decision.tree")
a = "12/12/2018 13:12"
type(a)
class(a)
for i in range(10){
print(i)
}
for i in range(10){
print(i)
}
for i in range(10){
print(i)
}
for i in range(10)
for i in range(10):
{
print(i)
}
for( i in range(10)):
{
print(i)
}
for( i in range(0,10)):
{
print(i)
}
range(10)
range(0,10)
range(0,10,1)
range(0:10)
seq(0,10)
corr = function(df,colm){
max_cor = c(0.0001,0.0002,0.0003)
names(max_cor) = c("abc", "def", "ghi")
cor_df = data.frame(matrix(1, nrow = ncol(df)))
for(coly in names(df)){
new_cor = 0
A1 = nrow(df)*sum((df[colm] * df[coly]));
A2 = sum(df[colm]);
B1 = sum(df[coly]);
B2 = nrow(df)*sum(df[colm] * df[colm]);
C = nrow(df)*sum(df[coly] * df[coly]);
cor = (A1 - A2*B1) / (sqrt( ( B2 - (A2)^2)*(C - (B1)^2) ));
cor_df[1,coly] = cor;
new_cor = abs(cor)
if(new_cor >= min(max_cor) && new_cor!= 1){
max_cor = replace(max_cor, max_cor==min(max_cor), new_cor);
names(max_cor)[max_cor==new_cor] = paste(colm, coly)
}
}
return (max_cor)
}
cor(Boston)
#INPUT
library(MASS)
data("Boston")
corr(Boston,"medv")
corr = function(df,colm){
max_cor = c(0.0001,0.0002,0.0003)
names(max_cor) = c("abc", "def", "ghi")
cor_df = data.frame(matrix(1, nrow = ncol(df)))
for(coly in names(df)){
new_cor = 0
A1 = nrow(df)*sum((df[colm] * df[coly]));
A2 = sum(df[colm]);
B1 = sum(df[coly]);
B2 = nrow(df)*sum(df[colm] * df[colm]);
C = nrow(df)*sum(df[coly] * df[coly]);
cor = (A1 - A2*B1) / (sqrt( ( B2 - (A2)^2)*(C - (B1)^2) ));
cor_df[1,coly] = cor;
new_cor = abs(cor)
if(new_cor >= min(max_cor) && new_cor!= 1){
max_cor = replace(max_cor, max_cor==min(max_cor), new_cor);
names(max_cor)[max_cor==new_cor] = paste(colm, coly)
}
}
return (max_cor)
}
library(MASS)
library(MASS)
data("Boston")
corr(Boston,"medv")
View(Boston)
corr(Boston,"tax")
cor(data)
corr(Boston,"tax")
cor(Boston)
corr(Boston,"dis")
corr(Boston,"black")
dataset = read.csv("Diabetes.csv")
setwd("C:/x/Docs/Rstudio weekday(Divyang)/R_Machine_Learning")
dataset = read.csv("Diabetes.csv")
set.seed(123)
id = sample(2, nrow(dataset), prob = c(0.7,0.3),
replace = T)
training_set = dataset[id==1,]
test_set = dataset[id ==2,]
library(rpart)
classifier = rpart(is_diabetic ~ ., data = training_set)
plot(classifier, margin = 0.1)
text(classifier, cex = 0.7) # cex is font
library(rpart.plot)
rpart.plot(classifier, type = 3, digits = 3, fallen.leaves = T, cex = 0.6)
library(randomForest)
set.seed(123)
rf = randomForest(is_diabetic ~ ., data = training_set)
print(rf)
table(dataset$is_diabetic)
dataset = read.csv("Diabetes.csv")
set.seed(123)
dataset = dataset[:,c(1:10)]
dataset = dataset[,c(1:10)]
dataset = dataset[,1:10]
1:10
dataset[,1:2]
dataset = dataset[,1:10]
View(dataset)
data = read.csv("Diabetes.csv")
set.seed(123)
dataset = data[,1:10]
dataset = data[1:10,]
library(randomForest)
set.seed(123)
rf = randomForest(is_diabetic ~ ., data = dataset)
print(rf)
attributes(rf)
attributes(rf)[6]
rf$votes
rf$err.rate
set.seed(123)
rf = randomForest(is_diabetic ~ ., data = dataset,
importance = T, mtry = 5, ntree = 10)
print(rf)
# plot classifier2
plot(rf)  # 3 lines, for different no of 'mtry'
rf$call
rf$type
rf$predicted
rf$votes
rf = randomForest(is_diabetic ~ ., data = dataset,
importance = T, mtry = 5, ntree = 10)
rf$predicted
rf$votes
rf = randomForest(is_diabetic ~ ., data = dataset,
importance = T, mtry = 5, ntree = 20)
rf$votes
rf$ntree
rf$inbag
rf$test
rf$forest
rf$err.rate
getTree(rf, labelVar = TRUE)
getTree(rf, k=2, labelVar = TRUE)
getTree(rf, k=20, labelVar = TRUE)
getTree(rf, k=21, labelVar = TRUE)
plot.rf.tree()
rf$predicted
rf$votes
rf$confusion
rf$oob.times
rf$test
rf$terms
rf$inbag
rf
rf = randomForest(is_diabetic ~ ., data = dataset,
importance = T, mtry = 3, ntree = 20)
rf
rf = randomForest(is_diabetic ~ ., data = dataset,
importance = T, mtry = 3, ntree = 20)
rf
rf = randomForest(is_diabetic ~ ., data = dataset,
importance = T, mtry = 3, ntree = 20)
rf
rf = randomForest(is_diabetic ~ ., data = dataset,
importance = T, mtry = 3, ntree = 20)
rf
View(dataset)
rf$predicted
rf
rf$predicted
rf$err.rate
predict(rf)
rf$votes
rf$oob.times
rf$classes
rf$importance
rf$localImportance
rf$proximity
rf$ntree
rf$inbag
rf$terms
rf$test
rf$y
rf$votes
rf
#pred
library(caret)
rf3 = randomForest(is_diabetic ~ ., data = training_set,
importance = T, mtry = 5, ntree = 500)
rf3 = randomForest(is_diabetic ~ ., data = dataset,
importance = T, mtry = 5, ntree = 500)
rf3
best = tuneRF(training_set[-9], training_set$is_diabetic,
stepFactor = 1, improve = 0.05, trace = T, plot = T)
best = tuneRF(dataset[-9], dataset$is_diabetic,
stepFactor = 1, improve = 0.05, trace = T, plot = T)
#Importance
importance(rf2) # check the importance of variables
#Importance
importance(rf3) # check the importance of variables
varImpPlot(rf3) #to visualize the important variables
classifier4 = tree(is_diabetic ~ ., data = dataset)
library(tree)
classifier4 = tree(is_diabetic ~ ., data = dataset)
plot (classifier4, margin = 0.1)
text(classifier4, cex = 0.7)
y_pred4 = predict(classifier4, newdata = test_set, type = "class")
rf
best = tuneRF(dataset[-9], dataset$is_diabetic,
stepFactor = 1, improve = 0.05, trace = T, plot = T)
best
best = tuneRF(dataset[-9], dataset$is_diabetic,
stepFactor = 1, improve = 0.01, trace = T, plot = T)
best
best = tuneRF(dataset[-9], dataset$is_diabetic,
stepFactor = 0.1, improve = 0.01, trace = T, plot = T)
best = tuneRF(dataset[-9], dataset$is_diabetic,
stepFactor = 1, improve = 0.01, trace = T, plot = T)
best = tuneRF(dataset[-9], dataset$is_diabetic,
stepFactor = 1.5, improve = 0.01, trace = T, plot = T)
rf
rf
rf$oob.times
rf$votes
rf
rf$err.rate
rf$votes
rf$forest
rf
